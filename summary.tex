% ========================================================
% ========================================================

\section{Math Prerequisites}

\subsection{Derivatives }

\begin{itemize}
	\item $\partial \mathbf{x}^T \mathbf{a} = \partial \mathbf{a}^T \mathbf{x} = \mathbf{a}$
	\item $\partial \*x^T\*A\*x = (\*A + \*A^T)\*x$
	\item $\partial \mathbf{a}^T \mathbf{X} \mathbf{b} = \partial \mathbf{a}^T \mathbf{X}^T \mathbf{b} = \mathbf{a} \mathbf{b}^T$
	\item $\partial \lVert \mathbf{x} \lVert^2_2 = 2\mathbf{x}$ and $\partial \lVert \mathbf{X} \lVert^2_2 = 2\mathbf{X}$
	\item $\partial \lVert \mathbf{x}-\mathbf{a} \lVert_2 = \frac{\mathbf{x}-\mathbf{a}}{\lVert \mathbf{x}-\mathbf{a} \lVert_2}$
	\item $\partial (\*b-\*A\*x)^T(\*b-\*A\*x) = -2\*A^T(\*b-\*A\*x)$
	\item \textbf{Chain rule}:
		\begin{myalign*}
			\frac{\partial g(\mathbf{U}))}{\partial \mathbf{X}_{ij}} = \frac{\partial g(f(\mathbf{X})))}{\partial \mathbf{X}_{ij}} = tr[(\frac{\partial g(\mathbf{U})}{\partial \mathbf{U} })^T\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}_{ij}}]
		\end{myalign*}

\end{itemize}

\subsection{Linear Algebra}

\begin{itemize}
	\item $a_0 \*x_0 + \hdots + a_n \*x_n = \*X^T \*a$, $\*x_n$ are col of $\*X^T$
    \item \textbf{positive definite} (pd) if $\*a^T \*V \*a > 0$
	\item $(\*x-\*b)^T (\*x-\*b) = \lVert \*x - \*b \lVert^2_2 $
	\item $\lVert \*A \lVert_F = \sqrt{\sum \sigma_i^2}$ , and $\lVert \*A \lVert_1 = tr({\sqrt{\*A^T \*A}}$)
	\item $\lVert \mathbf{X} \lVert_2 =\lVert \mathbf{X}^T \lVert_2$
\end{itemize}

\subsection{Distributions}
Valid distribution $p(x) > 0 \text{, }\forall x$ and $\sum p(x) = 1$
Model is identifiable iff $\theta_1 = \theta_2 \rightarrow P_{\theta_1} = P_{\theta_2}$
\begin{itemize}
    \item \textbf{Gaussian} (Not convex): $ \N(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(- \frac{(x - \mu)^2}{2 \sigma^2})$
     $ \N(x | \mu, \Sigma^2) = \frac{\exp(-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu))}{\sqrt{ (2 \pi)^D det(\Sigma)}} $
    \item \textbf{Poisson}: $\text{P(k events in interval)} = e^{-\lambda} \frac{\lambda^k}{k!}$
    \item \textbf{Bernoulli}:  $p(y | \mu) = \mu^y (1- \mu)^{1 - y}$
\end{itemize}

\subsection{Convexity}

A function $f(x)$ is convex if
 
\begin{itemize}

    \item for any $\*x_1, \*x_2 \in \* X$ and $0 \leq \lambda \leq 1$, we have :
        \begin{myalign*}
            f(\lambda \*x_1 + (1 - \lambda) \*x_2) \leq \lambda f(\*x_1) + (1 - \lambda) f(\*x_2)
        \end{myalign*}
   \item it is a sum of convex functions
   \item the Hessian $\*H$ is positive semi-definite
	
\end{itemize}


\subsection{Others}

\begin{itemize}
    \item Production of independent variables:
        \begin{myalign*}
            \V(xy) = \E(x^2)\E(y^2) - [\E(x)]^2 [\E(y)]^2
        \end{myalign*}
    \item Covariance matrix of a data vector $\*x$
   		\begin{myalign*}
   		    \*\Sigma = \frac{1}{N} \sum_{n = 1}^N (\*x_n - \E(\*x))(\*x_n - \E(\*x))^T
   		\end{myalign*}
    \item Multi-class $\*x$
   		\begin{myalign*}
   		    p(\* y | \*X, \beta) &= \prod^N p(\* y_n | \*x_n, \beta) \\
   		     &= \prod^K \prod^N [p(\* y_n = k| \*x_n, \beta)]^{\tilde{y}_{nk}}
   		\end{myalign*}
	
\end{itemize}

% ========================================================
% ========================================================
\section{Cost functions}
\begin{itemize}
    \item Cost functions are used to learn parameters that explain the data well.
    \item It is essential to make sure that a global minimum exist $\rightarrow$ lower bounded
\end{itemize}

\textbf{Mean square error (MSE)}:
\begin{myalign*}
    MSE(\bm w ) = \frac{1}{N} \sum^N_{n = 1} (y_n - f(\* x_n ))^2
\end{myalign*}

\begin{itemize}
    \item MSE is \textbf{convex} thus it has only one global minumum value.
    \item MSE is not good when outliers are present.
\end{itemize}

\textbf{Mean Absolute Error (MAE)}:
\begin{myalign*}
    MAE = \frac{1}{N} \sum^N_{n = 1} |y_n - f(\* x_n)|
\end{myalign*}

\textbf{Huber loss}
\begin{myalign*}
    Huber = 
    \left\{ 
        \begin{array}{c c}
            \frac{1}{2} z^2 &,|z| \leq \delta \\
            \delta |z| - \frac{1}{2} \delta^2 & ,|z| > \delta
        \end{array}
    \right.
\end{myalign*}
\begin{itemize}
\item Huber loss is convex, differentiable, and also robust to outliers but hard to set $\delta$.\\
\end{itemize}

\textbf{Tukey's bisquare loss}
\begin{myalign*}
    L(z) =
    \left\{ 
        \begin{array}{c c}
            z(\delta^2 - z^2)^2 &, |z| < \delta \\
            0 &, |z| \geq \delta
        \end{array}
    \right.
\end{myalign*}
Non-convex, non-diff., but robust to outliers.

\textbf{Hinge loss}: $[1 - y_n f(\* x_n)]_+ = \max(0, 1 - y_n f(\* x_n))$ \\
\textbf{Logistic loss}: $ \log(1 - \exp(y_n f(\* x_n)))$


% ========================================================

\section{Regression}
\begin{itemize}
	\item Model that assume linear relationship
	\begin{myalign*}
	  \*y_n & \approx f(\* x_n) := \*w_0 + \*w_1 \*x_{n1} + ... = \*w_0 + \*x^T_n \*w \\
			&\approx \tilde{\* x}_n^T \*w \text{ ,where $\tilde{x}$ contains offset comp.}
	\end{myalign*}
	
	
    \item \textbf{Prediction}: predict the ouput for a new input vector.
    \item \textbf{Interpretation}: understand the effect of inputs on output.

\end{itemize}

% ========================================================
\section{Optimization}
\subsection{Grid search}
\begin{itemize}
    \item Compute the cost over a grid of $M$ points to find the minimum. Exponential Complexity. Hard to find a good range of values
\end{itemize}

% ========================================================

\subsection{GD - Gradient Descent (Batch)}
\begin{itemize}
	\item GD uses only first-order information and takes steps in the opposite direction of the gradient
	\item Given cost function $\L(\*w)$ we want to find $\* w$
	\begin{myalign*}
	    \*w = \arg\min_{\*w} \L(\*w)
	\end{myalign*}
    \item Take steps in the opposite direction of the gradient
   	\begin{myalign*}
	    \*w^{(t + 1)} \leftarrow \*w^{(t)} - \gamma \bm \nabla \L(\*w^{(t)})
   	\end{myalign*}

    \item With $\gamma$ too big, method might diverge. With $\gamma$ too small, convergence is slow.
\end{itemize}


% ========================================================

\subsection{SGD - Stochastic Gradient Descent}

In ML, most cost functions are formulated as a sum over the training examples:
$$ \L(\*w) = \frac{1}{N}\sum_{n=1}^N\L_n(\*w)$$
$\Rightarrow$ SGD update rule (only n-th training exam.):
$$ \*w^{(t + 1)} \leftarrow \*w^{(t)} - \gamma \bm \nabla \L_n(\*w^{(t)})$$
\emph{Idea}: Cheap but unbiased estimate of grad. $$\E[\bm\nabla\L_n(\*w)] = \bm \nabla(\*w)$$

% ========================================================

\subsection{Mini-batch SGD}

Update direction ($B \subseteq [N]$):
$$\bm g^{(t)} := \frac{1}{|B|} \sum_{n\in B} \bm \nabla \L_n(\* w^{(t)}) $$ 
Update rule : $ \* w^{(t + 1)} \leftarrow \* w^{(t)} - \gamma \* g^{(t)}$ 

% ========================================================

\subsection{Gradients for MSE}
\begin{itemize}
    \item We define the error vector $\* e$:
    \begin{myalign*}
        \*e := \* y - \*{X} \*w
    \end{myalign*}
    \item and MSE as follows:
    \begin{myalign*}
        \L(\* w) = \frac{1}{2N} \sum^N_{n = 1} (\*y_n - \*{\tilde{x}}_n^T \* w)^2 = \frac{1}{2N} \*e^T \*e
    \end{myalign*}
    \item then the gradient is given by
    \begin{myalign*}
        \bm \nabla \L(\* w) = - \frac{1}{N} \*{X}^T \*e
    \end{myalign*}

    \item Optimality conditions:
    \begin{enumerate}
        \item \textit{necessary}: gradient equal zero: $\frac{d \L(\*w^*)}{d \*w} = 0$
        \item \textit{sufficient}: Hessian matrix is positive definite: $\* H(\*w^*) = \frac{d^2 \L(\*w^*)}{d \*w d \*w^T}$
    \end{enumerate}

    \item Very sensitive to illconditioning $\Rightarrow$ always normalize features.
    \item \textit{Complexity}: $O(NDI)$ with $I$ the number of iterations
\end{itemize}



\subsection{Subgradients (Non-Smooth OPT)}

A vector $\* g\in \R^D$ s.t.
$$ \L(\* u) \geq \L(\* w) + \* g^T(\* u - \* w) \quad \forall \* u \in \R^D $$
is the subgradient to $\L$ at $\* w$.
If $\L$ is differentiable at $\* w$, we have $\* g = \bm \nabla \L(\* w)$
% ========================================================

\subsection{Constrained Optimization}
Find solution $\min{\L(\*w)}$ s.t. $\*w \in \mathcal{C}$

\begin{itemize}
    \item Add proj. onto $\mathcal{C}$ after each step:
    \begin{myalign*}
    	P_{\mathcal{C}}(\*w') = \arg\min \lvert \*v-\*w' \lvert \text{, } \*v \in\mathcal{C}
    \end{myalign*}
    \begin{myalign*}
       	\*w^{(t+1)} = P_{\mathcal{C}}[\*w^{(t)} - \gamma \nabla \L (\*w^{(t)})]
    \end{myalign*}
    \item Use penalty functions
    \begin{itemize}
    \item $ \min \L(\*w) + I_{\mathcal{C}}, I_{\mathcal{C}}=0\text{ if } \*w \in \mathcal{C} \text{, ow } +\infty$
    \item $\min \L(\*w) + \lambda \lvert \*A\*w - \*b \lvert$
    \end{itemize}
    \item Stopping criteria when $\L(\*w)$ close to 0
    
\end{itemize}

% ========================================================

\section{Least Squares}
\begin{itemize}
   %\item In some cases, we can compute the minimum of the cost function analytically.

    \item Use the first optimality conditions:
    \begin{myalign*}
        \bm \nabla L(\* w^*)= 0 \Rightarrow \*X^T \*e = \*X^T (\*y - \*X \* w)=  0
    \end{myalign*}
    \item When $\*X^T\*X$ is invertible, we have the closed-form expression
    \begin{myalign*}
        \* w^* = (\*X^T\*X)^{-1} \*X^T \*y
    \end{myalign*}
    \item thus we can predict values for a new $\* x_m$
    \begin{myalign*}
        \*y_m := \*{x^T_m} \* w^* = \*{x^T_m}(\*X^T\*X)^{-1} \*X^T \*y
    \end{myalign*}
    \item The \textbf{Gram matrix} $\*X^T\*X$ is pd and is also invertible iff $\*X$ has full column rank.
    
    \item \textit{Complexity}: $O(ND^2 + D^3) \equiv O(ND^2)$

    \item $\*X$ can be rank deficient when $D > N$ or when the comlumns $\*{\bar{x}}_d$ are nearly collinear. $\Rightarrow$  matrix is ill-conditioned.
\end{itemize}

% ========================================================

\section{Maximum Likelihood (MLE)} 
\begin{itemize}
    \item Let define our mistakes $\epsilon_n \sim \N(0, \sigma^2)$.
    \begin{myalign*}
        \rightarrow \*y_n = \*{x}_n^T \* w + \epsilon_n
    \end{myalign*}    
    \item Another way of expressing this:
    \begin{myalign*}
        p(\*y | \*{X, \* w}) &= \prod_{n = 1}^N p(\* y_n | \*{x}_n, \*w)\\
        &= \prod_{n = 1}^N \N(\*y_n | \*{x}_n^T \* w, \sigma^2)
    \end{myalign*}
    which defines the likelihood of observating $\* y$ given $\*X$ and $\bm w$
    \item Define cost with log-likelihood
    \begin{myalign*}
        \L_{MLE}(\bm w) &= \log p(\*y | \*X, \*w)\\
        &= - \frac{1}{2 \sigma^2} \sum^N_{n = 1} (\*y_n - \*{x}_n^T \*w)^2 + cnst
    \end{myalign*}
    \item Maximum likelihood estimator (MLE) gives another way to design cost functions
    \begin{myalign*}
        \argmin_{\* w} \L_{MSE}(\* w) = \argmax_{\*w} \L_{MLE}(\*w)
    \end{myalign*}
    \item MLE can also be interpreted as finding the model under which the observed data is most likely to have been generated from.
    \item $\*w_{\text{MLE}} \rightarrow \*w_{\text{true}}$ for large amount of data

\end{itemize}


% ========================================================

\section{Ridge Regression (RR)}
\begin{itemize}
    \item Linear models usually overfit. We can penalize them with a \textbf{regularization term}
    \begin{myalign*}
        \min_{\bm w} \L(\* w) + \Omega(\*w) 
    \end{myalign*}    
    \item $L_2$-Reg. (Ridge): $\Omega(\*w) = \lambda \lVert \*w \lVert_2^2$
    \item $\rightarrow$ small values of $\*w_i$, not sparse
    \item $\rightarrow$ $\*w^{\star} = (\*X^T\*X + \lambda ' \*I)^{-1}\*X^T\*y$ with $\lambda ' = 2N\lambda$
    \item $\rightarrow$ No ill cond., $(\*X^T\*X + \lambda ' \*I)^{-1}$ exists
    \item $L_1$-Reg. (Lasso): $\Omega(\*w) = \lambda \lVert \*w \lVert_1$
    \item $\rightarrow$ large values of $\*w_i$, sparse
    \item {\bf Maximum-a-posteriori (MAP)}
    \item (i) Posterior prob. $\propto$ Likelihood $\times$ Prior prob
    \begin{myalign*}
        p(\*y | \*X \*w) &= \prod^N \N(\*y_n | \*x_n^T \*w, \sigma_n^2) \\
        p(\*w) &= \N(\*w | 0, \sigma_0^2 \*I_D) \\
        \text{then} \rightarrow \*w^{\star} &= \argmax_{\*w} p(\*y | \*X \*w) \cdot p(\*w)
    \end{myalign*}   
    \begin{myalign*}
        \*w^{\star} = \argmin_{\*w} \sum^N \frac{1}{2\sigma_n^2}(\*y_n - \*x^T \*w)^2 + \frac{1}{2\sigma_0^2} \lVert\*w \lVert^2
    \end{myalign*}   
\end{itemize}

% ========================================================

\section{Bias-Variance decomposition}
\begin{itemize}
	\item The expected test error can be expressed as the sum of two terms
	\begin{itemize}
	 	\item \textbf{Squared bias}: The average \textit{shift} of the predictions 
	 	\item \textbf{Variance}: measure how data points vary around their average.
	 \end{itemize} 
	 \begin{center}
	 	expected loss $= (\text{bias})^2$ + variance + noise
	 \end{center}
	\item Model bias and estimation bias are important
	\item RR increases estimation bias and reduces var
	\item Model more complex increases test error
	\item Small $\lambda \rightarrow$ low bias but large variance
	\item Large $\lambda \rightarrow$ large bias but low variance
	\item {\bf Simple} $\rightarrow$ large bias but low variance
	\item {\bf Complex} $\rightarrow$ low bias but large variance
	\begin{myalign*}
	    err = \sigma^2 + \E[f_{lse} - \E[f_{lse}]]^2 + [f_{true} - \E[f_{lse}]]^2
	\end{myalign*}
\end{itemize}
% ========================================================

\section{Logistic Regression}

\begin{itemize}
	\item \textbf{Classification} relates input variables $\*x$ to discrete output variable $\*y$
	\item \textbf{Binary classifier}: we use $y = 0$ for $\*C_1$ and $\*y = 1$ for $\*C_2$.
	\item Can use least-squares to predict $\hat{y}_*$
	\begin{myalign*}
	    \hat{y} = 
	    \left\{
	    	\begin{array}{c c}		
	    		\*C_1 & \hat{y}_* < 0.5 \\
	    		\*C_2 & \hat{y}_* \geq 0.5 \\
	    	\end{array}		
	    \right.
	\end{myalign*}
	\item \textbf{Logistic function}
	\begin{myalign*}
	    \sigma(x) = \frac{\exp(x)}{1 + \exp(x)}
	\end{myalign*}
	\begin{myalign*}
	    p(\*y_n = \*C_1| \*x_n) = \sigma( \*x^T \*w)\\
	    p(\*y_n = \*C_2 | \*x_n) = 1 - \sigma( \*x^T \*w)
	\end{myalign*}
	\item The probabilistic model:
	\begin{myalign*}
	    p(\*y | \*X, \* w) = \prod_{n = 1}^N \sigma( \*x_n^T \* w)^{\*y_n}(1 - \sigma(\* x_n^T \* w))^{1 - \*y_n}
	\end{myalign*}
	\item The log-likelihood (w.r.t. MLE):
	\begin{myalign*}
	    \L(\* w) &= - \sum_{n = 1}^N \*y_n \ln \sigma(\*x_n^T \*w)  + (1-\*y_n)\ln(1- \sigma(\*x_n^T \*w))\\
	    &=  \sum_{n = 1}^N  \ln[1 + \exp(\*x_n^T \* w) ]-  \*y_n \*x_n^T\*w
	\end{myalign*}
	\item We can use the fact that
	\begin{myalign*}
	    \frac{d}{dx}\log(1 + \exp(x)) = \sigma(x)
	\end{myalign*}
	\item Gradient of the log-likelihood %LEARN
	\begin{myalign*}
	    \*g = \bm \nabla \L (\* w) &= \sum_{n = 1}^N  \*x_n( \sigma(\* x_n^T \* w) - \*y_n) \\
	    &= \*X^T[\sigma(\*X \bm w) - \*y]
	\end{myalign*}
	\item The negative of the log-likelihood $- \L_{mle}(\bm w)$ is convex
	\item \textbf{Hessian} of the log-likelihood
	\begin{itemize}
		\item We know that
		\begin{myalign*}
		    \frac{d \sigma(t)}{dt} = \sigma(t)(1 - \sigma(t))
		\end{myalign*}
		\item Hessian is the derivative of the gradient
		\begin{myalign*}
		    \*H(\* w) &= - \frac{d \*g(\* w)}{d \* w^T}  = \sum_{n = 1}^N \frac{d}{d \* w^T} \* x_n \sigma(\* x_n^T \* w) \\
		    &= \sum_{n = 1}^N \* x_n \* x_n^T \sigma(\* x_n^T \* w)(1 - \sigma(\* x_n^T \* w)) \\
		    &= \tilde{\*X}^T \*S \tilde{\*X}
		\end{myalign*}
		where $\*S$ is a $N \times N$ diagonal matrix with diagonals
		\begin{myalign*}
		    S_{nn} = \sigma(\* x_n^T\* w)(1 - \sigma(\* x_n^T \* w))
		\end{myalign*}
		\item The negative of the log-likelihood is not strictly convex.
	\end{itemize}
	\item \textbf{Newton's Method} %LEARN
	\begin{itemize}
		\item Uses second-order information and takes steps in the direction that minimizes a quadratic approximation (Taylor)
		\begin{myalign*}
		    \L(\*w) = \L(\* w^{(k)}) + \*\nabla\L_k^T (\*w - \* w^{(k)})\\ + (\* w - \* w^{(k)})^T \*H_k(\* w - \* w^{(k)})
		\end{myalign*}
		and it's minimum is at
		\begin{myalign*}
		    \* w^{k + 1} =\* w^{(k)} - \gamma_k \*H_k^{-1}\*\nabla\L_k
		\end{myalign*}
		\item Complexity: $O((ND^2 + D^3)I)$
	\end{itemize}
	\item \textbf{Regularized Logistic Regression}
	\begin{myalign*}
	    \argmin_{\*w} 
	    	- \sum_{n = 1}^N \ln p(\*y_n | \*x_n^T \* w) + \frac{\lambda}{2} \lVert \*w \lVert^2
	    \end{myalign*}
\end{itemize}

% ========================================================
\section{Exponential family distribution \& Generalized Linear Model}


\begin{itemize}
	  \item Exponential family distribution
		\begin{myalign*}
		    p(\*y | \bm \eta) = h(y) \exp(\bm \eta^T \bm \phi(\*y) - A(\bm \eta))
		\end{myalign*}
      \item {\bf Bernoulli} distribution example
      \begin{myalign*}
	  \rightarrow \exp( \log(\frac{\mu}{1 - \mu})y + \log(1 - \mu)))
      \end{myalign*}
      (i) there is a relationship between $\eta$ and $\mu$ through the \textbf{link function}
      \begin{myalign*}
	  \eta = \log(\frac{\mu}{1 - \mu}) \leftrightarrow \mu = \frac{e^{\eta}}{1 + e^{\eta}}
      \end{myalign*}
      (ii) Note that $\mu$ is the mean parameter of $y$
      (iii) Relationship between the mean $\bm \mu$ and $\bm \eta$ is defined using a link function $g$
        \begin{myalign*}
            \bm \eta = \*g(\bm \mu) \Leftrightarrow \bm \mu = \*g^{-1}(\bm \eta)
        \end{myalign*}
      \item {\bf Gaussian} distribution example
      \begin{myalign*}
	  \exp( (\frac{\mu}{2\sigma^2}, \frac{-1}{2\sigma^2})(y, y^2)^T - \frac{\mu^2}{2\sigma^2}-\frac{1}{2}\ln(2\pi \sigma^2))
      \end{myalign*}
      (i) link function
      \begin{myalign*}
	  \eta = (\eta_1 = \mu/\sigma^2, \eta_2 = -1/(2\sigma^2))^T
      \end{myalign*}
      \begin{myalign*}
      	  \mu = -\eta_1/(2\eta_2) \text{ ; } \sigma^2 = -1/(2\eta_2)
      \end{myalign*}
      
  \item First and second derivatives of $A(\eta)$ are related to the mean and the variance
  \begin{myalign*}
      \frac{d A(\eta)}{d \eta} = \E[\bm \phi(\eta)], \hspace{4pt} \frac{d^2 A(\eta)}{d \eta^2} = \V[\bm \phi(\eta)]
  \end{myalign*}
  \item $A(\eta)$ is convex
  \item The generalized maximum likelihood cost to minimize is
  \begin{myalign*}
      \min_{\* w} \L(\* w) = - \sum_{n = 1}^N \log(p(\*y_n | \*x^T_n \* w))
  \end{myalign*}
  where $p(\*y_n | \* x^T_n \* w)$ is an exponential family distribution
  \item We obtain the solution
  \begin{myalign*}
      \frac{d \L}{d \* w} = \*X^T[\*g^{-1}(\*X \*w ) - \bm \phi(\*y)]
  \end{myalign*}
\end{itemize}

% ========================================================

\section{k-Nearest Neighbor (k-NN)}
\begin{itemize}
	\item The k-NN prediction for $\*x$ is
	\begin{myalign*}
	    f(\*x) = \frac{1}{k} \sum_{\*x_n \in nbh_k(\*x)} \*y_n
	\end{myalign*}
	where $nbh_k(\*x)$ is the neightborhood of $\*x$ defined by the k closest points $\*x_n$.
	\item \textbf{Curse of dimensionality}: Generalizing correctly becomes exponentially harder as the dimensionality grows.
	\item Gathering more inputs variables may be bad
\end{itemize}

% ========================================================

\section{Support Vector Machine} 
\begin{itemize}
	\item Combination of the kernel trick plus a modified loss function (Hinge loss)
	\item Solution to the dual problem is sparse and non-zero entries will be our \textbf{support vectors}.
	\item \textbf{Kernelised feature vector} where $\bm \mu_k$ are centroids
	\begin{myalign*}
	    \bm \phi(\*x) = [k(\*x, \bm \mu_1), ..., k(\*x, \bm \mu_K)]
	\end{myalign*}
	\item In practice we'll take a subset of data points to be prototype $\rightarrow$ \textbf{sparse vector machine}.
	\item Assume $y_n \in \{-1, 1\}$
	\item SVM optimizes the following cost
	\begin{myalign*}
	    \L(\* w) = \min_{\* w} \sum_{n = 1}^N [1 - \*y_n \tilde{\* \phi}_n^T \* w]_+ + \frac{\lambda}{2} \lVert \*w \lVert^2
	\end{myalign*}
	\item Minimum doesn't change with a rescaling of $\* w$
	\item choose the hyperplane so that the distance from it to the nearest data point on each side is maximized
	\item \textbf{Duality}:
	\begin{itemize}
		\item Hard to minimize $g(\* w)$ so we define
		\begin{myalign*}
		    \L(\* w) = \max_{\bm \alpha} G(\* w, \bm \alpha)
		\end{myalign*}
		\item we use the property that
		\begin{myalign*}
		    [\*v_n]_+ = \max(0, \*v_n) = \max_{\alpha_n \in [0, 1]} \alpha_n \*v_n
		\end{myalign*}
		\item We can rewrite the problem as
		\begin{myalign*}
		    \min_{\* w} \max_{\alpha} \sum_{n = 1}^N \alpha_n (1 - \*y_n \bm \phi_n^T \* w) + \frac{\lambda}{2} \lVert \*w \lVert_2^2 
		\end{myalign*}
		\item This is differentiable, convex in $\bm w$ and concave in $\bm \alpha$
		\item \textbf{Minimax theorem}: 
		\begin{myalign*}
		    \min_{\* w} \max_{\bm \alpha} G(\* w, \bm \alpha) = \max_{\bm \alpha} \min_{\* w} G(\* w, \bm \alpha)
		\end{myalign*}
		because $G$ is convex in $\* w$ and concave in $\bm \alpha$.
		\item Derivative w.r.t. $\* w$:
		\begin{myalign*}
		    \bm \nabla_{\* w} G(\* w, \bm \alpha) = - \sum_{n=1}^N\alpha_n \*y_n \* x_n + \lambda \* w
		\end{myalign*}
		\item Equating this to 0, we get:
			\begin{myalign*}
			  \* w(\bm \alpha) = \frac{1}{\lambda} \sum_{n=1}^N \alpha_n \*y_n \*x_n = \frac{1}{\lambda} \*{X}^T\*{Y}\bm \alpha \\
			  \*{Y} := \text{diag}(\bm y)
			\end{myalign*}
		\item Plugging $\* w^*$ back in the dual problem
		\begin{myalign*}
		    \max_{\bm \alpha \in [0, 1]^N} \bm \alpha^T \*1 - \frac{1}{2\lambda} \bm \alpha^T \*Y \* X  \*X^T \* Y \bm \alpha
		\end{myalign*}
		\item This is a differentiable least-squares problem. Optimization is easy using Sequential Minimal Optimization. It is also naturally kernelized with $\*K = \bm X^T \bm X$
		\item The solution $\bm \alpha$ is sparse and is non-zero only for the training examples that are instrumental in determining the decision boundary.
		\item $\bm \alpha$ is the slope of lines that are lower bound to Hinge loss
	\end{itemize}
\end{itemize}

\begin{itemize}
	\item {\bf Non support vector}: Example that lies on the correct side, outside margin $\bm \alpha_n = 0$
	\item {\bf Essen. support vector}: Example that lies on the margin $\bm \alpha_n \in (0,1)$
	\item {\bf Bound support vector}: Example that lies strictly inside the margin or wrong side $\bm \alpha_n = 1$
	\item Use Coordinates Descent to find $\bm \alpha$. Update one coordinate ($\argmin$) at the time and others constant.
\end{itemize}

% ========================================================

\section{Kernel Ridge Regression}
\begin{itemize}
	\item The following is true for ridge regression
	\begin{myalign*}
	    \* w^* &= (\*X^T \*X + \lambda \*I_D)^{-1} \*X^T \*y \text{ , (1)}\\
	    &= \*X^T(\*X\*X^T + \lambda \*I_N)^{-1} \*y = \*X^T \bm \alpha^* \text{ , (2)}
	\end{myalign*}
	\item Complexity of computing $\* w$: (1) $O(D^2 N + D^3)$, (2) $O(D N^2 + N^3)$
	\item Thus we have
	\begin{myalign*}
	     \*w^* = \*X^T\bm \alpha^*, \quad \text{with } \* w^* \in \R^D \text{ and } \bm \alpha^* \in \R^N
	\end{myalign*}
	\item The representer theorem allows us to write an equivalent optimization problem in terms of $\bm \alpha$.
	\begin{myalign*}
	    \bm \alpha = \argmax_{\bm \alpha} 
	    \left(
	    - \frac{1}{2}\bm \alpha^T (\*X^T \*X + \lambda \*I_N) \bm \alpha + \bm \alpha^T \*y 
	    \right)
	\end{myalign*}
	\item $\*K = \*X \*X^T$ is called the \textbf{kernel matrix} or \textbf{Gram matrix}.
	\item If $\*K$ is positive definite, then it's called a \textbf{Mercer Kernel}.
	\item $\*K_{i,j} = k(\*x_i, \*x_j)$
	\item If the kernel is Mercer, then there exists a function $\bm \phi(\*x)$ s.t.
	\begin{myalign*}
	    k(\*x, \*x') = \bm \phi(\*x)^T \bm \phi(\*x')
	\end{myalign*}
	\item \textbf{Kernel trick}: 
%	\begin{itemize}
%		\item We can work directly with $\*K$ and never have to worry about $\*X$
%		\item Replace $\langle \*x, \*x' \rangle$ with $k(\*x, \*x')$.
%		\item Kernel function can be interpreted as a measure of similarity
%		\item The evaluation of a kernel is usually faster with $k$ than with $\bm \phi$
%	\end{itemize}
%	\item Kernelized rigde regression might be computationally more efficient in some cases.
	\begin{itemize}
		\item compute dot-product in $\mathbb{R}^m$ while remaining in $\mathbb{R}^n$
		\item Replace $\langle \*x, \*x' \rangle$ with $k(\*x, \*x')$.
	\end{itemize}
	
	\item \textbf{Common Kernel}
	\begin{itemize}
		\item Polynomial Kernel: $(\gamma \langle \*x_i, \*x_j \rangle + r) ^d$
		\item Radial Basis function kernel (RBF)
			\begin{myalign*}
	    			 k(\*x, \*x') = \exp(- \frac{1}{2}(\*x - \*x')^T (\*x - \*x'))
			 \end{myalign*} 
		\item Sigmoid Kernel: $\tanh (\langle \*x_i, \*x_j \rangle + r)$ 
	\end{itemize}
	 
	\item \textbf{Properties of kernels} to ensure the existance of a corresponding $\bm \phi$:
	\begin{itemize}
		\item symmetric: $k(\*x, \*x') = k(\*x', \*x)$
		\item positive semi-definite.
	\end{itemize}
	\item Thus we get
	\begin{myalign*}
	    \*y = \*w^T \*x = \sum_{i = 1}^K \alpha_i \*x_i^T \*x = \sum_{i = 1}^K \alpha_i k(\*x, \*x_i) 
	\end{myalign*}
\end{itemize}

% ========================================================


\section{K-means}
\begin{itemize}
	\item \textbf{Unsupervised learning}: Represent particular input patterns in a way that reflects the statistical structure of the overall collections of input partterns.
	\item \textbf{Cluster} are groups of points whose inter-point distances are small compared to the distances outside the cluster.
	\begin{myalign*}
	    \min_{\*z, \bm \mu} \L(\*z, \bm \mu) = \sum_{k = 1}^K \sum_{n = 1}^N z_{nk} ||\*x_n - \bm \mu_k||^2_2
	\end{myalign*}
	such that $z_{nk} \in \{0, 1\}$ and $\sum_{k = 1}^K z_{nk} = 1$
	\item K-means algorithm (Coordinate Descent): \\
	Initialize $\bm \mu_k$, then iterate
	\begin{enumerate}
		\item For all n, compute $\*z_n$ given $\bm \mu$
		\begin{myalign*}
		    z_{nk} = 
		    \left\{
		    	\begin{array}{c c}
		    		1 & \text{ if } k = \argmin_j || \*x_n - \bm \mu ||^2_2\\
		    		0 & \text{otherwise}
		    	\end{array}
		    \right.
		\end{myalign*}
		\item For all $k$, compute $\mu_k$ given $\*z$
		\begin{myalign*}
		    \bm \mu_k = \frac{\sum_{n = 1}^N z_{nk} \*x_n}{\sum_{n = 1}^N z_{nk}}
		\end{myalign*}
	\end{enumerate}
	\item A good initialization procedure is to choose the prototypes to be equal to a random subset of $K$ data points.
	\item Probabilistic model
	\begin{myalign*}
	    & p(\*z, \bm \mu) = \prod_{n = 1}^N \prod_{k = 1}^K 
	    \left[
	    	\N(\*x_n | \bm \mu_k, \*I)
	    \right]^{z_{nk}} \\
	     & -\log{p(\* x_n | \mu, z)}= \sum^N \sum^K \frac{1}{2} \lVert \*x_n - \mu_k \lVert^2 \*z_{nk} + c'
	\end{myalign*}
	\item K-means as a Matrix Factorization
	\begin{myalign*}
	 \min_{\*z, \bm \mu} \L(\*z, \bm \mu) = ||\*X - \*M\*Z^T||_{\text{Frob}}^2
	\end{myalign*}
	\item Computation can be heavy, each example can belong to only on cluster and clusters have to be spherical.
\end{itemize}

% ========================================================

\section{Gaussian Mixture Models}
\begin{itemize}
	\item Clusters can be elliptical using a full covariance matrix instead of isotropic covariance.
	\begin{myalign*}
	    p(\*X | \bm \mu, \bm \Sigma, \*z) = \prod_{n = 1}^N \prod_{k = 1}^K 
		    \left[
		    	\N(\*x_n | \bm \mu_k, \* \bm \Sigma_k)
		    \right]^{z_{nk}}
	\end{myalign*}

	\item \textbf{Soft-clustering}: Points can belong to several cluster by defining $z_n$ to be a random variable.
	\begin{myalign*}
		p(z_{n} = k) &= \pi_k \text{ where } \pi_k > 0, \forall k, %LEARN
		\sum_{k = 1}^K \pi_k = 1
	\end{myalign*}
	\item Joint distribution of Gaussian mixture model
	\begin{myalign*}
	    p(\*X, \*z | \bm \mu, \bm \Sigma, \bm \pi)
	    = \prod_{n = 1}^N
	    	p(\*x_n | \*r_n, \bm \mu, \bm \Sigma) p(\*z_n | \bm \pi) \\
	    = \prod_{n=1}^N \prod_{k = 1}^K [(\N(\*x_n |\bm \mu_k, \bm \Sigma_k))^{z_{nk}}] \prod_{k = 1}^K [\pi_k]^{z_{nk}}
	\end{myalign*}
	\item $z_n$ are called \textit{latent} unobserved variables
	\item Unknown parameters are given by $ \bm \theta = \{\bm \mu, \bm \Sigma, \bm \pi\}$
	\item We get the \textbf{marginal likelihood} by marginalizing $z_n$ out from the likelihood
	\begin{myalign*}
	    p(\*x_n | \bm \theta) &= \sum_{k = 1}^K p(\*x_n, z_{n} = k | \bm \theta)\\
	    &= \sum_{k = 1}^K p(z_{n} = k | \bm \theta) p(\*x_n | z_{n} = k, \bm \theta)\\
	    &= \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)
	\end{myalign*}
	\item Without a latent variable model, number of parameters grow at rate $O(N)$
	\item After marginalization, the growth is reduced to $O(D^2 K)$

	\item To get maximum likelihood estimate of $\bm \theta$, we maximize
	\begin{myalign*}
	    \max_{\bm \theta} \sum_{n = 1}^N \log \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)
	\end{myalign*}
\end{itemize}

% ========================================================

\section{Expectation Maximization Algorithm} %LEARN

\begin{itemize}
	\item \textit{[ALGORITHM]} Start with $\bm \theta^{(1)}$ and iterate
	\begin{enumerate}
		\item \textit{Expectation step}: Compute a lower bound to the cost such that it is tight at the previous $\bm \theta^{(t)}$
		\begin{comment}
		\begin{myalign*}
		    \log \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k) \geq \sum_{k = 1}^K \gamma(r_{nk}) \log \frac{\pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)}{\gamma(r_{nk})}
		\end{myalign*}
		\end{comment}
		with equality when,
		\begin{myalign*}
		    q_{kn} = \frac{\pi_k \N(\*x_n| \bm \mu_k, \bm \Sigma_k)}{\sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)}
		\end{myalign*}
		\item \textit{Maximization step}: Update $\bm \theta$
		\begin{myalign*}
		    \bm \theta^{(t + 1)} = \argmax_{\bm \theta} \L(\bm \theta, \bm \theta^{(t)})
		\end{myalign*}

		\begin{myalign*}
		    \bm \mu_k^{(t + 1)} = \frac{\sum_{n = 1}^N \gamma^{(i)}(r_{nk}) \*x_n}{\sum_{n = 1}^N q_{kn}^{(t)}}
		\end{myalign*}

		\begin{myalign*}
		    \bm\Sigma_k^{(t + 1)} = \frac{\sum_{n = 1}^N q_{kn}^{(t)} (\*x_n - \bm \mu_k^{(t + 1)})(\*x_n - \bm \mu_k^{(t + 1)})^T}{\sum_{n = 1}^N q_{kn}^{(t)})}
		\end{myalign*}

		\begin{myalign*}
		    \pi_k^{(t + 1)} = \frac{1}{N} \sum_{n = 1}^N q_{kn}^{(t)}
		\end{myalign*}
	\end{enumerate}
	\item If covariance is diagonal $\rightarrow$ K-means.
\end{itemize}

% ========================================================

\section{Matrix factorization}
\begin{itemize}
	\item We have $D$ movies and $N$ users
	\item $\*X$ is a matrix $D \times N$ with $x_{dn}$ the rating of n'th user for d'th movie.
	\item We project data vectors $\*x_n$ to a smaller dimension $\*z_n \in \Bbb R^M$
	\item We have now 2 latent variables:
	\begin{itemize}
		\item $\*Z$ a $N \times K$ matrix that gives features for the users
	 	\item $\*W$ a $D \times K$ matrix that gives features for the movies
	 \end{itemize} 
	 \begin{myalign*}
	     x_{dn} \approx \*w_d^T\*z_n
	 \end{myalign*}
	 \item We can add a regularizer and minimize the following cost:
	 \begin{myalign*}
	     \L(\*W, \*Z) = \frac{1}{2}\sum_{(d,n)\in\Omega} [x_{dn} - (\*W\*Z^T)_{dn}]^2 \\ + \frac{\lambda_w}{2} \lVert \*W \lVert_{\text{Frob}}^2 + \frac{\lambda_z}{2} \lVert \*Z \lVert_{\text{Frob}}^2 
	 \end{myalign*}
	 \item {\bf SGD}: For one fixed element $(d,n)$ we derive entry $(d',k)$ of $\*W$ (if $d=d'$ oth. 0):
	 \begin{myalign*}
		\frac{\partial}{\partial w_{d',k}} f_{d,n}(\*W, \*Z)  = -[x_{dn}-(\*W\*Z^T)_{dn}]z_{nk}
	 \end{myalign*}
	 And of $\*Z$ (if $n=n'$ oth. 0):
	 \begin{myalign*}
	 	\frac{\partial}{\partial z_{n',k}} f_{d,n}(\*W, \*Z)  = -[x_{dn}-(\*W\*Z^T)_{dn}]w_{nk}
	 \end{myalign*}
	 \begin{myalign*}
  		\*W^{t+1} &= \*W^t - \gamma \nabla_w f_{d,n}(\*W^t, \*Z^t) \\
 	 	\*Z^{t+1} &= \*W^t - \gamma \nabla_z f_{d,n}(\*W^t, \*Z^t)
	 \end{myalign*}
	 \item We can use coordinate descent algorithm, by first minimizing w.r.t. $\*Z$ given $\*W$ and then minimizing $\*W$ given $\*Z$. This is called \textbf{Alternating least-squares (ALS)}:
	 \begin{myalign*}
	     \*Z^T &\leftarrow (\*W^T \*W + \lambda_z \*I_K)^{-1} \*W^T \*X \\
	     \*W^T &\leftarrow (\*Z^T \*Z + \lambda_w \*I_K)^{-1} \*Z^T\*X^T
	 \end{myalign*}
	 \item \textit{Complexity}: $O(D N K^2 + N K^3) \rightarrow O(D N K^2)$

\end{itemize}
% ========================================================

\section{Singular Value Decomposition}
\begin{itemize}
	\item Matrix factorization method
	\begin{myalign*}
	    \*X = \*U \*S \*V^T
	\end{myalign*}
	\begin{itemize}
		\item $\*U$ is a unitary $D \times D$ matrix
		\item $\*V$ is a unitary $N \times N$ matrix
		\item $\*S$ is a non-negative diagonal matrix of size $D \times N$ which are called \textbf{singular values} appearing in a descending order.
		\item Columns of $\*U$ and $\*V$ are the left and right \textbf{singular vectors} respectively.
	\end{itemize}
	\item Assuming $D < N$ we have
	\begin{myalign*}
	    \*X = \sum_{d = 1}^D s_d \*u_d \*v_d^T
	\end{myalign*}
	This tells you about the spectrum of $\*X$ where higher singular vectors contain the \textit{low-frequency information} and lower singular values contain the \textit{high-frequency information}. 
	\item {\bf Truncated SVD}: \\
	Take the matrix $\*S^{(K)}$ with the $K$ first diagonal elements non zero. Then, rank-$K$ approx:	
	\begin{myalign*}
		\*X \approx \*X_K = \*U\*S^{(K)}\*V^T
	\end{myalign*}
	
\end{itemize}
% ========================================================

\section{Principal Componement Analysis}
\begin{itemize}
	\item PCA is a dimensionality reduction method and a method to decorrelate the data
$\*X \approx \tilde{\*X} = \*W \*Z^T$
such that columns of $\*W$ are orthogonal.
\item If the data is zero mean
\begin{myalign*}
    \*\Sigma = \frac{1}{N} \*X \*X^T &\Rightarrow \*X \*X^T = \*U \*S^2 \*U^T \\
    \Rightarrow \*U^T \*X \*X^T \*U &= \*U^T \*U \*S^2 \*U^T \*U = \*S^2\\
\end{myalign*}
\item Thus the columns of matrix $\*U$ are called the \textbf{principal components} and they decorrelate the covariance matrix.
\item Using SVD, we can compute the matrices in the following way
\begin{myalign*}
    \*W &= \*U \*S_D^{1 / 2}, \*Z^T = \*S^{1 / 2}\*V^T 
\end{myalign*}
\item Not invariant under scalings of the feature = arbitrariness, $\rightarrow$ normalize $\*X$
\end{itemize}



\section{Neural Net}
\begin{itemize}
	\item Basic structure: 
	One \emph{input} layer of size D, L \emph{hidden} layers of size K, and one \emph{output} layer. (\emph{feedforward} network). $x_j^{(l)} = \phi\left(\sum_i w_{i,j}^{(l)}x_i^{(l-1)} + b_j^{(l)} \right)$.
	\item NN can represent the Rienmann sum with only two layers $\Rightarrow$ It's powerful!
	\item Cost function: \\$\L = \frac{1}{N} \sum_{n=1}^N\left( y_n - f^{(L+1)} \circ ... \circ f^{(1)}(\bm x_n^{(0)})\right)^2$ \\
	We can use SGD to minimize the cost function.
\end{itemize}

% ========================================================

\subsection{Backpropagation Algorithm}
\begin{itemize}
 \item \emph{Forward pass}: Compute $\bm z^{(l)} = \left(\bm W^{(l)}\right)^T\bm x^{(l-1)} + \bm b^{(l)}$ with $\bm x^{(0)} = \bm x_n$ and $\bm x^{(l)} = \phi(\bm z^{(l)})$.
 \item \emph{Backward pass}: Set $\delta^{(L+1)} = -2(y_n - \bm x^{(L+1)})\phi'(z^{(L+1)})$ (if squared loss). Then compute
 
\end{itemize}
 \begin{myalign*}
  \delta_j^{(l)} = \frac{\partial \L_n}{\partial z_j^{(l)}} = \sum_k \frac{\partial \L_n}{\partial z_k^{(l+1)}}\frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} \\= \sum_k \delta_k^{(l+1)}\bm W_{j,k}^{(l+1)}\phi'(z_j^{(l)}) 
 \end{myalign*}
\begin{itemize}
 \item \emph{Final Computation}:
\end{itemize}
\begin{myalign*}
\frac{\partial\L_n}{\partial w_{i,j}^{(l)}} = \sum_k \frac{\partial \L_n}{\partial z_k^{(l)}}\frac{\partial z_k^{(l)}}{\partial w_{i,j}^{(l)}} = \frac{\partial \L_n}{\partial z_j^{(l)}}\frac{\partial z_j^{(l)}}{\partial w_{i,j}^{(l)}} \\= \delta_j^{(l)}\bm x_i^{(l-1)}
\end{myalign*}
\begin{myalign*}
\frac{\partial \L_n}{\partial b_j^{(l)}} = \sum_k \frac{\partial \L_n}{\partial z_k^{(l)}}\frac{\partial z_k^{(l)}}{\partial b_j^{(l)}} = \frac{\partial \L_n}{\partial z_j^{(l)}}\frac{\partial z_j^{(l)}}{\partial b_j^{(l)}} \\= \delta_j^{(l)}\cdot 1 = \delta_j^{(l)} 
\end{myalign*}
 
% ======================================================== 
 
\subsection{Activation Functions}
\begin{description}
 \item[Sigmoid] $\phi(x) = \frac{1}{1+e^{-x}}$ Positive, bounded. $\phi'(x) \simeq 0$ for large $|x|$ $\Rightarrow$ Learning slow.
 \item[Tanh] $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \phi(2x) - 1/2$. Balanced, bounded. Learning slow too.
 \item[ReLU] $(x)_{+} = \max{0,x}$ Positive, unbounded. Derivate = 1 if $x>0$, 0 if $x<0$
 \item[Leaky ReLU] $f(x) = \max{\alpha x, x}$ Remove 0 derivative.
 \item[Maxout] $f(x) = max{\bm x^T\bm w_1 + b_1, ..., \bm x^T\bm w_k + b_k}$ (Generalization of ReLU)
\end{description}

% ======================================================== 
 
\subsection{Convolutional NN} 

Sparse connections and \textit{weights sharing}: reduce complexity. (e.g. pixels in pictures only depend on neighbours)

% ======================================================== 
 
\subsection{Reg, Data Augmentation and Dropout}

\begin{itemize}
	\item Regularization term: $\frac{1}{2} \sum_{l=1}^{L+1} \mu^{(l)} || W ^{(l)} ||  ^{2} _{F}$
	\item Weight decay is $\Theta[t](1-\eta \mu)$ in:
	\begin{myalign*}
	\Theta[t+1] = \Theta[t] + \eta (\nabla \L + \mu \Theta[t])
	\end{myalign*}
	\item Data Augm.: e.g. shift or rotation of pics
	\item Dropout: avoid overfit. Drop nodes randomly. (Then average multiple drop-NN)
\end{itemize}

% ========================================================

\section{Bayes Net}
\begin{itemize}
	\item Graph example: $p(x, y, z) = p(y | x) p(z | x) p(x)$ : $(y \leftarrow x \rightarrow z)$ 
	\item \textbf{D-Separation} X and Y are D-separated by Z if every path from $ x \in X$ to $y \in Y$ is blocked by Z. ($\rightarrow$ independent)
	\item \textbf{Blocked Path} contains a variable that
	\begin{itemize}
		\item is in Z and is \textbf{head-to-tail} or \textbf{tail-to-tail}
		\item the node is \textbf{head-to-head} and neither the node nor the descendant are in Z.
	\end{itemize}	
	\item \textbf{Markov Blanket} (which blocks node A from the rest of the net) contains:
	\begin{itemize}
		\item parents of A
		\item children of A
		\item parents of children of A
	\end{itemize}
	
\end{itemize}






